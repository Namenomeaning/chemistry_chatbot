"""Generation node: Generate final response."""

from typing import Dict, Any, List
from langchain_core.messages import AIMessage, HumanMessage, BaseMessage
from ..state import AgentState
from ..schemas import FinalResponse
from ...services import gemini_service
from ...core.logging import setup_logging

logger = setup_logging(__name__)


def _get_conversation_context(messages: List[BaseMessage], max_turns: int = 2) -> str:
    """Extract recent conversation history for context.

    Args:
        messages: List of conversation messages
        max_turns: Maximum number of Q&A turns to include

    Returns:
        Formatted string with recent conversation history
    """
    if not messages or len(messages) < 2:
        return ""

    # Get last few messages (exclude current - we want previous context)
    recent = messages[-(max_turns * 2 + 1):-1] if len(messages) > max_turns * 2 else messages[:-1]

    if not recent:
        return ""

    context_parts = []
    for msg in recent:
        if isinstance(msg, HumanMessage):
            content = str(msg.content)[:200]
            context_parts.append(f"User: {content}")
        elif isinstance(msg, AIMessage):
            # Truncate but keep key info from previous answers
            content = str(msg.content)[:600]
            context_parts.append(f"CHEMI: {content}")

    return "\n".join(context_parts) if context_parts else ""


def generate_response(state: AgentState) -> Dict[str, Any]:
    """Generate final response with RAG context or direct LLM knowledge.

    For specific compound queries: Uses RAG context for image/audio
    For general knowledge queries: LLM answers directly (no RAG needed)

    Args:
        state: Current agent state

    Returns:
        Updated state with final_response
    """
    try:
        needs_rag = state.get("needs_rag", True)
        rag_context = state.get("rag_context", [])

        # Handle general knowledge queries (skip RAG)
        if not needs_rag:
            return _generate_direct_response(state)

        # Prepare RAG context (minimal schema: type, doc_id, iupac_name, formula, image_path, audio_path)
        rag_text = ""

        if rag_context:
            for i, doc in enumerate(rag_context, 1):
                doc_id = doc.get('doc_id', 'N/A')
                score = doc.get('score', 0.0)
                item_type = doc.get('type', 'unknown')
                rag_text += f"\nK·∫øt qu·∫£ {i} (ƒë·ªô kh·ªõp: {score:.2f}):\n"
                rag_text += f"- T√™n: {doc.get('iupac_name', 'N/A')}\n"
                rag_text += f"- C√¥ng th·ª©c: {doc.get('formula', 'N/A')}\n"
                rag_text += f"- Lo·∫°i: {item_type}\n"
                rag_text += f"- ID: {doc_id}\n"

        # Get original query and conversation history
        original_query = state.get("input_text", "") or state.get("rephrased_query", "")
        messages = state.get("messages", [])
        conversation_history = _get_conversation_context(messages)

        # Build conversation context section
        history_section = ""
        if conversation_history:
            history_section = f"""
L·ªäCH S·ª¨ H·ªòI THO·∫†I (quan tr·ªçng - KH√îNG l·∫∑p l·∫°i th√¥ng tin ƒë√£ n√≥i):
{conversation_history}
---"""

        prompt = f"""B·∫°n l√† CHEMI - gia s∆∞ H√≥a h·ªçc th√¢n thi·ªán cho h·ªçc sinh trung h·ªçc ph·ªï th√¥ng, gi√∫p c√°c em h·ªçc danh ph√°p IUPAC qu·ªëc t·∫ø.
{history_section}
Input:
- C√¢u h·ªèi hi·ªán t·∫°i: {original_query}
- K·∫øt qu·∫£ t√¨m ki·∫øm:{rag_text if rag_text else "\n(Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£)"}

QUY T·∫ÆC QUAN TR·ªåNG:
1. N·∫øu user y√™u c·∫ßu "th√™m th√¥ng tin", "chi ti·∫øt h∆°n", "c√≤n g√¨ n·ªØa" ‚Üí B·ªî SUNG th√¥ng tin M·ªöI, KH√îNG l·∫∑p l·∫°i nh·ªØng g√¨ ƒë√£ n√≥i
2. Th√¥ng tin b·ªï sung c√≥ th·ªÉ bao g·ªìm:
   - T√≠nh ch·∫•t v·∫≠t l√Ω (nhi·ªát ƒë·ªô s√¥i, nhi·ªát ƒë·ªô n√≥ng ch·∫£y, m√†u s·∫Øc, m√πi)
   - T√≠nh ch·∫•t h√≥a h·ªçc (ph·∫£n ·ª©ng ƒë·∫∑c tr∆∞ng, kh·∫£ nƒÉng ph·∫£n ·ª©ng)
   - ·ª®ng d·ª•ng th·ª±c t·∫ø trong ƒë·ªùi s·ªëng
   - Ph∆∞∆°ng ph√°p ƒëi·ªÅu ch·∫ø
   - L·ªãch s·ª≠ ph√°t hi·ªán
   - Vai tr√≤ trong c∆° th·ªÉ/m√¥i tr∆∞·ªùng

PHONG C√ÅCH TR·∫¢ L·ªúI:

1. S·ª¨A T√äN TI·∫æNG VI·ªÜT ‚Üí IUPAC nh·∫π nh√†ng (ch·ªâ l·∫ßn ƒë·∫ßu):
   - N·∫øu user d√πng "Natri" ‚Üí "√Ä, ƒë√¢y l√† **Sodium** nh√©!"
   - N·∫øu ƒë√£ gi·ªõi thi·ªáu t√™n IUPAC tr∆∞·ªõc ƒë√≥ ‚Üí kh√¥ng c·∫ßn nh·∫Øc l·∫°i

2. H∆Ø·ªöNG D·∫™N C√ÅCH PH√ÅT √ÇM (phi√™n √¢m ti·∫øng Vi·ªát) - ch·ªâ khi ch∆∞a n√≥i:
   - Sodium ‚Üí "üé§ C√°ch ƒë·ªçc: **s√¢u-ƒëi-·∫ßm**"
   - Ethanol ‚Üí "üé§ C√°ch ƒë·ªçc: **√©t-th·ªù-nol**"

3. TH√îNG TIN CHI TI·∫æT (s·ª≠ d·ª•ng ki·∫øn th·ª©c H√≥a h·ªçc):
   - Nguy√™n t·ªë: S·ªë hi·ªáu, c·∫•u h√¨nh electron, v·ªã tr√≠ b·∫£ng tu·∫ßn ho√†n, t√≠nh ch·∫•t ƒë·∫∑c tr∆∞ng
   - H·ª£p ch·∫•t: C√¥ng th·ª©c, c·∫•u tr√∫c, t√≠nh ch·∫•t, ·ª©ng d·ª•ng, ƒëi·ªÅu ch·∫ø

4. G·ª¢I √ù C√ÇU H·ªéI TI·∫æP THEO:
   - Cu·ªëi c√¢u tr·∫£ l·ªùi: "ü§î B·∫°n mu·ªën t√¨m hi·ªÉu th√™m v·ªÅ [g·ª£i √Ω c·ª• th·ªÉ] kh√¥ng?"

Output:
- text_response: C√¢u tr·∫£ l·ªùi th√¢n thi·ªán (markdown), B·ªî SUNG th√¥ng tin m·ªõi n·∫øu l√† follow-up
- selected_doc_id: ID t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm
- should_return_image: true (m·∫∑c ƒë·ªãnh)
- should_return_audio: true (m·∫∑c ƒë·ªãnh)
"""

        # Log conversation context for debugging
        if conversation_history:
            logger.info(f"Generate - has conversation history ({len(messages)} messages)")
        else:
            logger.info("Generate - no conversation history (first query)")

        # Call Gemini 2.5 Flash (best quality for final answer generation)
        logger.info("Generate - calling Gemini API with FinalResponse schema")
        response: FinalResponse = gemini_service.generate_structured(
            prompt=prompt,
            response_schema=FinalResponse,
            temperature=0.3,
            model="gemini-2.5-flash"
        )
        logger.info("Generate - Gemini API call succeeded")

        # Check if response is valid
        if response is None:
            logger.error("Generate - Gemini API returned None")
            return {
                "final_response": {
                    "text_response": "Xin l·ªói, ƒë√£ c√≥ l·ªói khi x·ª≠ l√Ω ph·∫£n h·ªìi t·ª´ h·ªá th·ªëng.",
                    "image_path": None,
                    "audio_path": None
                },
                "messages": [AIMessage(content="Xin l·ªói, ƒë√£ c√≥ l·ªói khi x·ª≠ l√Ω ph·∫£n h·ªìi t·ª´ h·ªá th·ªëng.")]
            }

        logger.debug(f"Generate - response type: {type(response)}, selected_doc_id: {getattr(response, 'selected_doc_id', 'MISSING')}")

        # Get file paths from documents (respect LLM decisions)
        image_path = None
        audio_path = None

        if response.selected_doc_id and rag_context:
            # Find matching document
            for doc in rag_context:
                if doc.get("doc_id") == response.selected_doc_id:
                    # Return local filesystem paths for Gradio
                    if response.should_return_image and doc.get("image_path"):
                        image_path = doc['image_path']
                    if response.should_return_audio and doc.get("audio_path"):
                        audio_path = doc['audio_path']
                    break

        logger.info(f"Generate - selected_doc: '{response.selected_doc_id}', has_image: {bool(image_path)}, has_audio: {bool(audio_path)}")

        # Append assistant's response to conversation history
        return {
            "final_response": {
                "text_response": response.text_response,
                "image_path": image_path,
                "audio_path": audio_path
            },
            "messages": [AIMessage(content=response.text_response)]
        }
    except Exception as e:
        logger.error(f"Generate node error: {str(e)}", exc_info=True)
        raise


def _generate_direct_response(state: AgentState) -> Dict[str, Any]:
    """Generate response directly from LLM knowledge (no RAG).

    Used for general knowledge queries like:
    - List queries: "danh s√°ch nh√≥m 7A", "c√°c halogen"
    - General properties: "t√≠nh ch·∫•t c·ªßa kim lo·∫°i ki·ªÅm"
    - Comparisons: "so s√°nh alkane v√† alkene"
    - Theory questions: "li√™n k·∫øt h√≥a h·ªçc l√† g√¨"

    Args:
        state: Current agent state

    Returns:
        Updated state with final_response (no image/audio)
    """
    query = state.get("rephrased_query", "")
    original_query = state.get("input_text", "") or query
    messages = state.get("messages", [])
    conversation_history = _get_conversation_context(messages)

    # Build conversation context section
    history_section = ""
    if conversation_history:
        history_section = f"""
L·ªäCH S·ª¨ H·ªòI THO·∫†I (KH√îNG l·∫∑p l·∫°i th√¥ng tin ƒë√£ n√≥i):
{conversation_history}
---"""

    prompt = f"""B·∫°n l√† CHEMI - gia s∆∞ H√≥a h·ªçc th√¢n thi·ªán cho h·ªçc sinh ph·ªï th√¥ng, gi√∫p c√°c em h·ªçc danh ph√°p IUPAC qu·ªëc t·∫ø.
{history_section}
C√¢u h·ªèi hi·ªán t·∫°i: {original_query}

NHI·ªÜM V·ª§: Tr·∫£ l·ªùi t·ª´ ki·∫øn th·ª©c H√≥a h·ªçc. KH√îNG c·∫ßn tra c·ª©u c∆° s·ªü d·ªØ li·ªáu.

QUY T·∫ÆC QUAN TR·ªåNG:
- N·∫øu user y√™u c·∫ßu "th√™m th√¥ng tin" ‚Üí B·ªî SUNG th√¥ng tin M·ªöI, kh√¥ng l·∫∑p l·∫°i
- S·ª≠ d·ª•ng ki·∫øn th·ª©c H√≥a h·ªçc ƒë·ªÉ cung c·∫•p th√¥ng tin chi ti·∫øt, ch√≠nh x√°c

PHONG C√ÅCH TR·∫¢ L·ªúI:

1. S·ª¨A T√äN TI·∫æNG VI·ªÜT ‚Üí IUPAC nh·∫π nh√†ng (ch·ªâ l·∫ßn ƒë·∫ßu):
   - "Theo chu·∫©n IUPAC qu·ªëc t·∫ø, m√¨nh d√πng t√™n [t√™n IUPAC] thay v√¨ [t√™n Vi·ªát] nh√©!"

2. D√ôNG T√äN IUPAC + PHI√äN √ÇM TI·∫æNG VI·ªÜT:
   - VD: "Sodium (s√¢u-ƒëi-·∫ßm)", "Methane (me-th√™n)", "Fluorine (flo-rin)"

3. ƒê·ªäNH D·∫†NG PH√ô H·ª¢P:
   - DANH S√ÅCH ‚Üí B·∫£ng markdown
   - T√çNH CH·∫§T ‚Üí Gi·∫£i th√≠ch ng·∫Øn g·ªçn, c√≥ v√≠ d·ª•
   - SO S√ÅNH ‚Üí B·∫£ng so s√°nh r√µ r√†ng
   - L√ù THUY·∫æT ‚Üí Gi·∫£i th√≠ch d·ªÖ hi·ªÉu cho l·ªõp 11

4. G·ª¢I √ù TI·∫æP THEO:
   - Cu·ªëi c√¢u tr·∫£ l·ªùi: "ü§î B·∫°n mu·ªën t√¨m hi·ªÉu th√™m v·ªÅ [g·ª£i √Ω c·ª• th·ªÉ] kh√¥ng?"

Output:
- text_response: C√¢u tr·∫£ l·ªùi th√¢n thi·ªán (markdown), B·ªî SUNG th√¥ng tin m·ªõi n·∫øu l√† follow-up
- selected_doc_id: null
- should_return_image: false
- should_return_audio: false
"""

    logger.info(f"Generate (direct) - query: '{query[:50]}...'")

    response: FinalResponse = gemini_service.generate_structured(
        prompt=prompt,
        response_schema=FinalResponse,
        temperature=0.3,
        model="gemini-2.5-flash"
    )

    if response is None:
        logger.error("Generate (direct) - Gemini API returned None")
        return {
            "final_response": {
                "text_response": "Xin l·ªói, ƒë√£ c√≥ l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi.",
                "image_path": None,
                "audio_path": None
            },
            "messages": [AIMessage(content="Xin l·ªói, ƒë√£ c√≥ l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi.")]
        }

    logger.info(f"Generate (direct) - response length: {len(response.text_response)}")

    return {
        "final_response": {
            "text_response": response.text_response,
            "image_path": None,  # No image for general knowledge queries
            "audio_path": None   # No audio for general knowledge queries
        },
        "messages": [AIMessage(content=response.text_response)]
    }
