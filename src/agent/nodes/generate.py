"""Generation node: Generate final response."""

from typing import Dict, Any
from langchain_core.messages import AIMessage
from ..state import AgentState
from ..schemas import FinalResponse
from ...services import gemini_service
from ...core.logging import setup_logging

logger = setup_logging(__name__)


def generate_response(state: AgentState) -> Dict[str, Any]:
    """Generate final response with RAG context or direct LLM knowledge.

    For specific compound queries: Uses RAG context for image/audio
    For general knowledge queries: LLM answers directly (no RAG needed)

    Args:
        state: Current agent state

    Returns:
        Updated state with final_response
    """
    try:
        needs_rag = state.get("needs_rag", True)
        rag_context = state.get("rag_context", [])

        # Handle general knowledge queries (skip RAG)
        if not needs_rag:
            return _generate_direct_response(state)

        # Prepare RAG context (minimal schema: type, doc_id, iupac_name, formula, image_path, audio_path)
        rag_text = ""

        if rag_context:
            for i, doc in enumerate(rag_context, 1):
                doc_id = doc.get('doc_id', 'N/A')
                score = doc.get('score', 0.0)
                item_type = doc.get('type', 'unknown')
                rag_text += f"\nKáº¿t quáº£ {i} (Ä‘á»™ khá»›p: {score:.2f}):\n"
                rag_text += f"- TÃªn: {doc.get('iupac_name', 'N/A')}\n"
                rag_text += f"- CÃ´ng thá»©c: {doc.get('formula', 'N/A')}\n"
                rag_text += f"- Loáº¡i: {item_type}\n"
                rag_text += f"- ID: {doc_id}\n"

        # Get original query to detect Vietnamese naming
        original_query = state.get("input_text", "") or state.get("rephrased_query", "")

        prompt = f"""Báº¡n lÃ  CHEMI - gia sÆ° HÃ³a há»c thÃ¢n thiá»‡n cho há»c sinh trung há»c phá»• thÃ´ng, giÃºp cÃ¡c em há»c danh phÃ¡p IUPAC quá»‘c táº¿.

Input:
- CÃ¢u há»i gá»‘c: {original_query}
- Káº¿t quáº£ tÃ¬m kiáº¿m:{rag_text if rag_text else "\n(KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£)"}

PHONG CÃCH TRáº¢ Lá»œI (quan trá»ng!):

1. Sá»¬A TÃŠN TIáº¾NG VIá»†T â†’ IUPAC nháº¹ nhÃ ng:
   - Náº¿u user dÃ¹ng "Natri" â†’ má»Ÿ Ä‘áº§u: "Ã€, Ä‘Ã¢y lÃ  **Sodium** nhÃ©! Theo chuáº©n IUPAC quá»‘c táº¿, mÃ¬nh dÃ¹ng tÃªn nÃ y thay vÃ¬ 'Natri' nha ğŸ˜Š"
   - Náº¿u user dÃ¹ng "Sáº¯t/Káº½m/Äá»“ng" â†’ "TÃªn quá»‘c táº¿ lÃ  **Iron/Zinc/Copper** nha!"
   - Náº¿u user dÃ¹ng "Metan" â†’ "TÃªn IUPAC lÃ  **Methane** nhÃ©!"

2. HÆ¯á»šNG DáºªN CÃCH PHÃT Ã‚M (phiÃªn Ã¢m tiáº¿ng Viá»‡t):
   - Sodium â†’ "ğŸ¤ CÃ¡ch Ä‘á»c: **sÃ¢u-Ä‘i-áº§m**"
   - Iron â†’ "ğŸ¤ CÃ¡ch Ä‘á»c: **ai-á»n**"
   - Ethanol â†’ "ğŸ¤ CÃ¡ch Ä‘á»c: **Ã©t-thá»-nol**"
   - Methane â†’ "ğŸ¤ CÃ¡ch Ä‘á»c: **me-thÃªn**"
   - Hydrogen â†’ "ğŸ¤ CÃ¡ch Ä‘á»c: **hai-Ä‘rá»-giáº§n**"
   - Oxygen â†’ "ğŸ¤ CÃ¡ch Ä‘á»c: **Ã³c-xi-giáº§n**"

3. Gá»¢I Ã NGHE AUDIO:
   - LuÃ´n thÃªm: "ğŸ’¡ *Máº¹o: Nghe audio vá»›i tá»‘c Ä‘á»™ 0.5x Ä‘á»ƒ nghe rÃµ cÃ¡ch phÃ¡t Ã¢m nhÃ©!*"

4. Gá»¢I Ã CÃ‚U Há»I TIáº¾P THEO:
   - Cuá»‘i cÃ¢u tráº£ lá»i: "ğŸ¤” Báº¡n cÃ³ muá»‘n tÃ¬m hiá»ƒu thÃªm vá» [tÃ­nh cháº¥t hÃ³a há»c/á»©ng dá»¥ng/pháº£n á»©ng Ä‘áº·c trÆ°ng] cá»§a [tÃªn cháº¥t] khÃ´ng?"

5. THÃ”NG TIN CÆ  Báº¢N (chÃ­nh xÃ¡c):
   - NguyÃªn tá»‘: KÃ½ hiá»‡u, sá»‘ hiá»‡u nguyÃªn tá»­, cáº¥u hÃ¬nh electron
   - Há»£p cháº¥t: TÃªn IUPAC, cÃ´ng thá»©c phÃ¢n tá»­, cÃ´ng thá»©c cáº¥u táº¡o, phÃ¢n loáº¡i

Output:
- text_response: CÃ¢u tráº£ lá»i thÃ¢n thiá»‡n (markdown) vá»›i phiÃªn Ã¢m vÃ  gá»£i Ã½
- selected_doc_id: ID tá»« káº¿t quáº£ tÃ¬m kiáº¿m
- should_return_image: true (máº·c Ä‘á»‹nh true Ä‘á»ƒ há»c sinh xem cáº¥u trÃºc)
- should_return_audio: true (máº·c Ä‘á»‹nh true Ä‘á»ƒ há»c sinh nghe phÃ¡t Ã¢m)
"""

        # Call Gemini 2.5 Flash (best quality for final answer generation)
        logger.info("Generate - calling Gemini API with FinalResponse schema")
        response: FinalResponse = gemini_service.generate_structured(
            prompt=prompt,
            response_schema=FinalResponse,
            temperature=0.3,
            model="gemini-2.5-flash"
        )
        logger.info("Generate - Gemini API call succeeded")

        # Check if response is valid
        if response is None:
            logger.error("Generate - Gemini API returned None")
            return {
                "final_response": {
                    "text_response": "Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i khi xá»­ lÃ½ pháº£n há»“i tá»« há»‡ thá»‘ng.",
                    "image_path": None,
                    "audio_path": None
                },
                "messages": [AIMessage(content="Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i khi xá»­ lÃ½ pháº£n há»“i tá»« há»‡ thá»‘ng.")]
            }

        logger.debug(f"Generate - response type: {type(response)}, selected_doc_id: {getattr(response, 'selected_doc_id', 'MISSING')}")

        # Get file paths from documents (respect LLM decisions)
        image_path = None
        audio_path = None

        if response.selected_doc_id and rag_context:
            # Find matching document
            for doc in rag_context:
                if doc.get("doc_id") == response.selected_doc_id:
                    # Return local filesystem paths for Gradio
                    if response.should_return_image and doc.get("image_path"):
                        image_path = doc['image_path']
                    if response.should_return_audio and doc.get("audio_path"):
                        audio_path = doc['audio_path']
                    break

        logger.info(f"Generate - selected_doc: '{response.selected_doc_id}', has_image: {bool(image_path)}, has_audio: {bool(audio_path)}")

        # Append assistant's response to conversation history
        return {
            "final_response": {
                "text_response": response.text_response,
                "image_path": image_path,
                "audio_path": audio_path
            },
            "messages": [AIMessage(content=response.text_response)]
        }
    except Exception as e:
        logger.error(f"Generate node error: {str(e)}", exc_info=True)
        raise


def _generate_direct_response(state: AgentState) -> Dict[str, Any]:
    """Generate response directly from LLM knowledge (no RAG).

    Used for general knowledge queries like:
    - List queries: "danh sÃ¡ch nhÃ³m 7A", "cÃ¡c halogen"
    - General properties: "tÃ­nh cháº¥t cá»§a kim loáº¡i kiá»m"
    - Comparisons: "so sÃ¡nh alkane vÃ  alkene"
    - Theory questions: "liÃªn káº¿t hÃ³a há»c lÃ  gÃ¬"

    Args:
        state: Current agent state

    Returns:
        Updated state with final_response (no image/audio)
    """
    query = state.get("rephrased_query", "")

    # Get original query
    original_query = state.get("input_text", "") or query

    prompt = f"""Báº¡n lÃ  CHEMI - gia sÆ° HÃ³a há»c thÃ¢n thiá»‡n cho há»c sinh phá»• thÃ´ng, giÃºp cÃ¡c em há»c danh phÃ¡p IUPAC quá»‘c táº¿.

CÃ¢u há»i: {original_query}

NHIá»†M Vá»¤: Tráº£ lá»i tá»« kiáº¿n thá»©c HÃ³a há»c. KHÃ”NG cáº§n tra cá»©u cÆ¡ sá»Ÿ dá»¯ liá»‡u.

PHONG CÃCH TRáº¢ Lá»œI:

1. Sá»¬A TÃŠN TIáº¾NG VIá»†T â†’ IUPAC nháº¹ nhÃ ng (náº¿u user dÃ¹ng tÃªn Viá»‡t):
   - "Theo chuáº©n IUPAC quá»‘c táº¿, mÃ¬nh dÃ¹ng tÃªn [tÃªn IUPAC] thay vÃ¬ [tÃªn Viá»‡t] nhÃ©!"

2. LUÃ”N DÃ™NG TÃŠN IUPAC + PHIÃŠN Ã‚M TIáº¾NG VIá»†T khi nháº¯c Ä‘áº¿n cháº¥t:
   - VD: "Sodium (sÃ¢u-Ä‘i-áº§m)", "Methane (me-thÃªn)", "Fluorine (flo-rin)"

3. Äá»ŠNH Dáº NG PHÃ™ Há»¢P:
   - DANH SÃCH â†’ Báº£ng markdown, thÃªm cá»™t "CÃ¡ch Ä‘á»c"
   - TÃNH CHáº¤T â†’ Giáº£i thÃ­ch ngáº¯n gá»n, cÃ³ vÃ­ dá»¥
   - SO SÃNH â†’ Báº£ng so sÃ¡nh rÃµ rÃ ng
   - LÃ THUYáº¾T â†’ Giáº£i thÃ­ch dá»… hiá»ƒu cho lá»›p 11
   - QUY Táº®C â†’ TrÃ¬nh bÃ y tá»«ng bÆ°á»›c

4. Gá»¢I Ã TIáº¾P THEO:
   - Cuá»‘i cÃ¢u tráº£ lá»i, gá»£i Ã½: "ğŸ¤” Báº¡n muá»‘n CHEMI tÃ¬m hiá»ƒu chi tiáº¿t vá» [gá»£i Ã½ liÃªn quan] khÃ´ng?"

Output:
- text_response: CÃ¢u tráº£ lá»i thÃ¢n thiá»‡n (markdown) vá»›i phiÃªn Ã¢m
- selected_doc_id: null
- should_return_image: false
- should_return_audio: false
"""

    logger.info(f"Generate (direct) - query: '{query[:50]}...'")

    response: FinalResponse = gemini_service.generate_structured(
        prompt=prompt,
        response_schema=FinalResponse,
        temperature=0.3,
        model="gemini-2.5-flash"
    )

    if response is None:
        logger.error("Generate (direct) - Gemini API returned None")
        return {
            "final_response": {
                "text_response": "Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i khi xá»­ lÃ½ cÃ¢u há»i.",
                "image_path": None,
                "audio_path": None
            },
            "messages": [AIMessage(content="Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i khi xá»­ lÃ½ cÃ¢u há»i.")]
        }

    logger.info(f"Generate (direct) - response length: {len(response.text_response)}")

    return {
        "final_response": {
            "text_response": response.text_response,
            "image_path": None,  # No image for general knowledge queries
            "audio_path": None   # No audio for general knowledge queries
        },
        "messages": [AIMessage(content=response.text_response)]
    }
